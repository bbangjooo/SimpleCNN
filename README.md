### Code-Description

구글링해보니까 CNN의 정확성을 올릴 때
- weight 초기화
- batch nomalization (질문 !!)
- dropout (질문 !!)

있다 해서 적용해봤씀

### result
`Test set: Average loss: 0.0270, Accuracy: 9914/10000 (99%)`

### Questions

1. layer들에서 `Conv2d`,`BatchNorm2d`, `ReLU` 이렇게 거치는데 각각 거칠 때의 효과?? 왜 하는지?
2. `Dropout` ?? 


### TODO

비쥬얼라이즈해봐야겠따

